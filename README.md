# kc-nlp
kc-nlp


להיט! הנה בדיוק “מה המטרה” של הסט הזה ומה כדאי להסיק להבא — קצר, ממוקד, ושימושי:

למה עושים את התרגיל הזה?

להכיר את אבני היסוד של NLP ב-spaCy: זיהוי ישויות (NER), לממטיזציה, עצירת מילות תפל (stopwords), התאמת ביטויי מפתח (PhraseMatcher), וסיווג תחבירי (POS).

להבין את ה-pipeline של spaCy ומה כל רכיב מייצר, כדי שתוכל להרכיב זרימה מתאימה למשימה אמיתית.

לנוע משימוש “ספרייתי” לשימוש “משימתי”: איך לתרגם פקודות API לתהליך חזרת־ערך (extract → normalize → match → visualize).

מה למדנו מכל חלק (ולמה זה חשוב בפועל)

NER: חילוץ ישויות מובְנות (PERSON, GPE, DATE) לבניית טבלאות/דוחות/טריגרים.
לקח: אל תסמוך בעיניים עצומות—בדיקת דוגמאות ידנית + התאמת המודל לשפה/תחום.

סינון לפי תווית (PERSON): איך להפוך NER לכלי ממוקד (למשל רשימת אנשים בלבד).
לקח: תמיד סנן לפי ent.label_ כדי לקבל “אות יחסית לרעש”.

Lemmatization: נרמול מילים לצורת בסיס לפני ספירה/חיפוש/השוואה.
לקח: לממטיזציה קריטית ל-matching ולטקסט־קלסיפיקציה; עבד עם token.lemma_.lower().

הסרת Stopwords: ניקוי “מילים חסרות אינפורמציה” לפני ספירה/וקטוריזציה.
לקח: זהירות עם שלילות (“not”, “no”)—אל תסיר שלילה במודלי סנטימנט/כוונה.

Stopword מותאם: הוספת מילים “שחוקות” בדומיין שלך כדי לשפר אות מול רעש.
לקח: עצב stoplist לפי הקורפוס שלך, לא רק לפי ברירת מחדל.

PhraseMatcher: איתור ביטויים מדויקים/רב-מילים (כמו “artificial intelligence”).
לקח: השתמש ב-attr="LOWER" כדי להתעלם מאותיות רישיות; ל־patterns חכמים יותר שקול Matcher או EntityRuler.

POS + הסבר: נותן שכבת תחביר שמאפשרת כללים (למשל תבניות “שם עצם + תואר”).
לקח: POS נהדר לכללי חיפוש מדויקים יותר מאשר “מחרוזת בלבד”.

DisplaCy: ויזואליזציה לדיבאג מהיר של תלות/תויות.
לקח: הסתכלות ויזואלית מקצרת זמן דיבאג והבנת כשלים במודל/בכללים.

מסקנות מעשיות להבא (“עשה ואל תעשה”)

עשה ✅

בחר מודל לשפה הנכונה (אנגלית: en_core_web_sm/md/lg; עברית: he_core_news_sm וכו’).

הרץ NER/POS לפני הסרה/ניקוי אגרסיביים; נקה אחר-כך לתכלית האנליטית.

למטלות אצווה/גדולות: השתמש ב-nlp.pipe לחיסכון זמן; כבה רכיבים מיותרים עם nlp.disable_pipes.

ב-PhraseMatcher השתמש ב-attr="LOWER" וצרף וריאנטים נחוצים (ריבוי/יחיד, מקף).

אל תעשה ❌

לא להסיר מילים שליליות בעיבוד סנטימנט/כוונה.

לא לערבב שפות עם מודל אנגלי ולצפות ל-NER תקין בעברית.

לא להסתמך רק על text.lower() במקום Lemma (זה מפספס הטיות/זמנים).

איך לנצל את זה בפרויקט “אמיתי”

הגדרת מטרה: מה מחלצים (אנשים? תאריכים? סכומים?)

בחירת מודל + בדיקה על דגימות אמת.

זרימה: doc = nlp(text) → NER/POS → Lemma/סינון → PhraseMatcher/EntityRuler → יצוא ל-DataFrame.

הערכה: דגימת Precision/Recall ידנית/אוטומטית על 50–100 משפטים.

טיוב: הוספת כללים לביטויים, הרחבת stoplist דומייני, ושיפור ניקוי.

לאן להתקדם עכשיו

להוסיף EntityRuler עם דפוסים משלך (כותרות, מוצרים, מחלקות).

לבנות צינורית שמחלצת PERSON + DATE מכל משפט ויוצרת טבלה “מי-מתי-איפה”.

להחליף למודל בינוני/גדול (md/lg) כשצריך NER מדויק יותר (על חשבון גודל/זמן).

סיכום קצר

מטרת התרגיל היא להכיר את רכיבי הבסיס של spaCy ולתרגל אותם על בעיות נפוצות: חילוץ ישויות, נרמול טקסט, סינון רעש, התאמת ביטויי מפתח, וניתוח תחבירי. להבא, בחר מודל לפי השפה והדומיין, נקה חכם (לא עיוור), השתמש ב-Lemma ו-PhraseMatcher עם כללי קייס, והערך תוצאות על דגימות אמת לפני הרחבה.
